{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zl9TAufw9Qd1"
   },
   "source": [
    "# Emotion Classification in texts using LSTM and Word2Vec\n",
    "\n",
    "### Architecture: \n",
    "(X) Text -> Embedding (W2V pretrained on wikipedia articles) -> Deep Network (CNN 1D) -> Fully connected (Dense) -> Output Layer (Softmax) -> Emotion class (Y)\n",
    "\n",
    "#### Embedding Layer\n",
    "* Word Embedding is a representation of text where words that have the similar meaning have a similar representation. We will use 300 dimentional word vectors pre-trained on wikipedia articles. We can also train the w2v model with our data, however our dataset is quite small and trained word vectors might not be as good as using pretrained w2v.\n",
    "\n",
    "#### Deep Network\n",
    "* Though text data is one-dimensional, we can use 1D convolutional neural networks to extract features from our data. The result of each convolution will fire when a special pattern is detected. By varying the size of the kernels and concatenating their outputs, you‚Äôre allowing yourself to detect patterns of multiples sizes (2, 3, or 5 adjacent words). Patterns could be expressions like ‚ÄúI hate‚Äù, ‚Äúvery good‚Äù and therefore CNNs can identify them in the sentence regardless of their position. \n",
    "\n",
    "#### Fully Connected Layer\n",
    "* The fully connected layer takes the deep representation from the RNN/LSTM/GRU and transforms it into the final output classes or class scores. This component is comprised of fully connected layers along with batch normalization and optionally dropout layers for regularization.\n",
    "\n",
    "#### Output Layer\n",
    "* Based on the problem at hand, this layer can have either Sigmoid for binary classification or Softmax for both binary and multi classification output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RfCuL7z79Qd2"
   },
   "source": [
    "## Workflow: \n",
    "1. Import Data\n",
    "2. Prepare the input data\n",
    "3. Import pre-trained W2V\n",
    "4. Create Neural Network Pipeline\n",
    "5. Train The Model\n",
    "6. Evaluate results\n",
    "\n",
    "\n",
    "\n",
    "üëã  **Let's start** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DqaIRlkz9Qd3"
   },
   "source": [
    "## 1. Import Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qmCVkrRM9Qd4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# text preprocessing\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "# plots and metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "# preparing input to our model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# keras layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cxWYzq4x9Qd8"
   },
   "source": [
    "Defining vector space dimension and fixed input size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "igVFBA5Q9Qd9"
   },
   "outputs": [],
   "source": [
    "# Number of labels: joy, anger, fear, sadness, neutral\n",
    "num_classes = 5\n",
    "\n",
    "# Number of dimensions for word embedding\n",
    "embed_num_dims = 300\n",
    "\n",
    "# Max input length (max number of words) \n",
    "max_seq_len = 500\n",
    "\n",
    "class_names = ['joy', 'fear', 'anger', 'sadness', 'neutral']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dGFcQUeW9Qd_"
   },
   "source": [
    "Importing our training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "a72__Tmn9QeA"
   },
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('data_train2.csv', encoding='utf-8')\n",
    "data_test = pd.read_csv('data_test2.csv', encoding='utf-8')\n",
    "\n",
    "X_train = data_train.Text\n",
    "X_test = data_test.Text\n",
    "\n",
    "y_train = data_train.Emotion\n",
    "y_test = data_test.Emotion\n",
    "\n",
    "data = data_train.append(data_test, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "id": "JXn25PDmKpJC",
    "outputId": "ba793bb8-dafb-4e48-a329-120d39821604"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral    21745\n",
       "joy         8548\n",
       "sadness     4332\n",
       "fear        4279\n",
       "anger       2975\n",
       "Name: Emotion, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.Emotion.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "id": "5LcgtHJX9QeC",
    "outputId": "10605cc0-a5a4-442b-86b1-ee12f126a2e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral    106592\n",
      "joy         41625\n",
      "sadness     20268\n",
      "fear        19998\n",
      "anger       13739\n",
      "Name: Emotion, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Just so you know Barack Obama will be here in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>cant fall asleep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>I missed the bl***y bus!!!!!!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>neutral</td>\n",
       "      <td>feels strong contractions but wants to go out....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Emotion                                               Text\n",
       "0  neutral  Just so you know Barack Obama will be here in ...\n",
       "1  neutral  @tiffanylue i know  i was listenin to bad habi...\n",
       "2  neutral  @dannycastillo We want to trade with someone w...\n",
       "3  neutral                                   cant fall asleep\n",
       "4  neutral                    I missed the bl***y bus!!!!!!!!\n",
       "5  neutral  feels strong contractions but wants to go out...."
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data.Emotion.value_counts())\n",
    "data.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSCuLZUB9QeG"
   },
   "source": [
    "## 2. Prepare input data\n",
    "To input the data to our NN Model we'll need some preprocessing:\n",
    "1. Tokenize our texts and count unique tokens\n",
    "2. Padding: each input (sentence or text) has to be of the same lenght\n",
    "3. Labels have to be converted to integeres and categorized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gFNYMo_9QeH"
   },
   "source": [
    "Basic preprocessing and tokenization using nltk to double check that sentences are properly split into words.\n",
    "We could also add stopword removal but steps like stemming or lemmatization are not needed since we are using word2vec and words with the same stem can have a different meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "FthEb_UP9QeH"
   },
   "outputs": [],
   "source": [
    "def clean_text(data):\n",
    "    \n",
    "    # remove hashtags and @usernames\n",
    "    data = re.sub(r\"(#[\\d\\w\\.]+)\", '', data)\n",
    "    data = re.sub(r\"(@[\\d\\w\\.]+)\", '', data)\n",
    "    \n",
    "    # tekenization using nltk\n",
    "    data = word_tokenize(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "id": "FyLWmVCShD_F",
    "outputId": "bfc65023-5b03-48d8-bbef-5b22da28e6ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> import nltk\n",
    ">>> nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pt-Fbxpk9QeK"
   },
   "source": [
    "*Making things easier for keras tokenizer üôÉ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "5NtG1Kqq9QeL"
   },
   "outputs": [],
   "source": [
    "texts = [' '.join(clean_text(text)) for text in data.Text]\n",
    "\n",
    "texts_train = [' '.join(clean_text(text)) for text in X_train]\n",
    "texts_test = [' '.join(clean_text(text)) for text in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "4qtB4-ew9QeO",
    "outputId": "05c5d9e2-777b-42e7-959c-e1ff93b34080"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yeah , plus it 's just a little too far to walk to Pinocchio 's for sushi\n"
     ]
    }
   ],
   "source": [
    "print(texts_train[92])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PyujkKyM9QeQ"
   },
   "source": [
    "**Tokenization + fitting using keras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "f1pBg_K09QeQ",
    "outputId": "c9cdc815-3fb5-4cd4-a838-e70f97ae7073"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 58249\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "sequence_train = tokenizer.texts_to_sequences(texts_train)\n",
    "sequence_test = tokenizer.texts_to_sequences(texts_test)\n",
    "\n",
    "index_of_words = tokenizer.word_index\n",
    "\n",
    "# vacab size is number of unique words + reserved 0 index for padding\n",
    "vocab_size = len(index_of_words) + 1\n",
    "\n",
    "print('Number of unique words: {}'.format(len(index_of_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48UfzGYF9QeT"
   },
   "source": [
    "**Padding** -> each input has the same length\n",
    "\n",
    "We defined maximun number of words for our texts and input size to our model has to be fixed - padding with zeros to keep the same input lenght (longest input in our dataset is ~250 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "id": "uBPUY1EU9QeT",
    "outputId": "83acbd19-3187-4a08-c985-3f459686302c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,     5,   662,  3854],\n",
       "       [    0,     0,     0, ...,    31,    72,   413],\n",
       "       [    0,     0,     0, ...,    44,    47,    42],\n",
       "       ...,\n",
       "       [    0,     0,     0, ...,  1844,     3,  1019],\n",
       "       [    0,     0,     0, ...,     1,    18,   239],\n",
       "       [    0,     0,     0, ..., 22213,   154,    17]], dtype=int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pad = pad_sequences(sequence_train, maxlen = max_seq_len )\n",
    "X_test_pad = pad_sequences(sequence_test, maxlen = max_seq_len )\n",
    "\n",
    "X_train_pad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "la-ajsyI9QeV"
   },
   "source": [
    "**Categorize** labels: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "JUAYMvow9QeW"
   },
   "outputs": [],
   "source": [
    "encoding = {\n",
    "    'joy': 0,\n",
    "    'fear': 1,\n",
    "    'anger': 2,\n",
    "    'sadness': 3,\n",
    "    'neutral': 4\n",
    "}\n",
    "\n",
    "# Integer labels\n",
    "y_train = [encoding[x] for x in data_train.Emotion]\n",
    "y_test = [encoding[x] for x in data_test.Emotion]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "id": "zkAyh7oY9QeZ",
    "outputId": "8a9ca9b7-de5b-4750-e437-f3646b7dfc69"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       ...,\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tFTVR9lC9Qeb"
   },
   "source": [
    "## 2. Import pretrained word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2L1XuTU9Qec"
   },
   "source": [
    "* Importing pretrained word2vec from file and creating embedding matrix\n",
    "* We will later map each word in our corpus to existing word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "G27E6atx9Qec"
   },
   "outputs": [],
   "source": [
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    with open(filepath) as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdk5sqFd9Qee"
   },
   "source": [
    "You can download and import any pre-trained word embeddings. I will use 300 dimentional w2v pre-trained on wikipedia articles. Download fast text english vectors: https://fasttext.cc/docs/en/english-vectors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "id": "GrQheHGX9Qef",
    "outputId": "ede8e798-c77f-4bc9-fc4d-98a57dd9bb1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading word vectors...\n",
      "Unzipping...\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "fname = 'embeddings/wiki-news-300d-1M.vec'\n",
    "\n",
    "if not os.path.isfile(fname):\n",
    "    print('Downloading word vectors...')\n",
    "    urllib.request.urlretrieve('https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip',\n",
    "                              'wiki-news-300d-1M.vec.zip')\n",
    "    print('Unzipping...')\n",
    "    with zipfile.ZipFile('wiki-news-300d-1M.vec.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('embeddings')\n",
    "    print('done.')\n",
    "    \n",
    "    os.remove('wiki-news-300d-1M.vec.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "xt4iGX6l9Qeh",
    "outputId": "bade905c-0485-48fc-ee4f-19f16d1c72cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58250, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedd_matrix = create_embedding_matrix(fname, index_of_words, embed_num_dims)\n",
    "embedd_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubs7e9tv9Qej"
   },
   "source": [
    "Some of the words from our corpus were not included in the pre-trained word vectors. If we inspect those words we'll see that it's mostly spelling errors. It's also good to double check the noise in our data f.e different languages or tokenizer errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "id": "HbcxXEOK9Qek",
    "outputId": "4a8b968f-f831-43e3-b092-d2cb16f1d91f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words found in wiki vocab: 40266\n",
      "New words found: 17983\n"
     ]
    }
   ],
   "source": [
    "# Inspect unseen words\n",
    "new_words = 0\n",
    "\n",
    "for word in index_of_words:\n",
    "    entry = embedd_matrix[index_of_words[word]]\n",
    "    if all(v == 0 for v in entry):\n",
    "        new_words = new_words + 1\n",
    "\n",
    "print('Words found in wiki vocab: ' + str(len(index_of_words) - new_words))\n",
    "print('New words found: ' + str(new_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mStq8Fp39Qem"
   },
   "source": [
    "## 3. Create CNN Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xJ7xg7Z9Qen"
   },
   "source": [
    "### Embedding Layer\n",
    "\n",
    "We will use pre-trained word vectors. We could also train our own embedding layer if we don't specify the pre-trained weights \n",
    "\n",
    "* **vocabulary size:** the maximum number of terms that are used to represent a text: e.g. if we set the size of the ‚Äúvocabulary‚Äù to 1000 only the first thousand terms most frequent in the corpus will be considered (and the other terms will be ignored)\n",
    "* **the maximum length:** of the texts (which must all be the same length)\n",
    "* **size of embeddings:** basically, the more dimensions we have the more precise the semantics will be, but beyond a certain threshold we will lose the ability of the embedding to define a coherent and general enough semantic area\n",
    "* **trainable:** True if you want to fine-tune them while training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "w_WTYPqb9Qen"
   },
   "outputs": [],
   "source": [
    "# Embedding layer before the actaul BLSTM \n",
    "embedd_layer = Embedding(vocab_size,\n",
    "                         embed_num_dims,\n",
    "                         input_length = max_seq_len,\n",
    "                         weights = [embedd_matrix],\n",
    "                         trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPmYswZA9Qeq"
   },
   "source": [
    "### Model Pipeline\n",
    "- the input is the first N words of each text (with proper padding)\n",
    "- the first level creates embedding of words, using vocabulary with a certain dimension, and a given size of embeddings\n",
    "- we will use 1D convolutional neural network to extract features from our data. The result of each convolution will fire when a special pattern is detected. By varying the size of the kernels and concatenating their outputs, you‚Äôre allowing yourself to detect patterns of multiples sizes (2, 3, or 5 adjacent words)\n",
    "- the output level has a number of neurons equal to the classes of the problem and a ‚Äúsoftmax‚Äù activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "f0tIQF4N9Qeq"
   },
   "outputs": [],
   "source": [
    "# Convolution\n",
    "kernel_size = 2\n",
    "filters = 512\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedd_layer)\n",
    "model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "id": "hvYf0u0b9Qes",
    "outputId": "a93f4ab3-509d-44b5-b959-a05cabbdbce0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 500, 300)          17475000  \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 499, 512)          307712    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 17,915,325\n",
      "Trainable params: 440,325\n",
      "Non-trainable params: 17,475,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTYVNROp9Qeu"
   },
   "source": [
    "## 3. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "id": "GKwv1evA9Qev",
    "outputId": "f1cd6e23-ca1f-4540-9026-94efcecc15f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "627/627 [==============================] - 2124s 3s/step - loss: 0.8944 - accuracy: 0.6618 - val_loss: 0.8319 - val_accuracy: 0.6835\n",
      "Epoch 2/6\n",
      "226/627 [=========>....................] - ETA: 20:34 - loss: 0.7747 - accuracy: 0.7059"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "epochs = 6\n",
    "\n",
    "hist = model.fit(X_train_pad, y_train, \n",
    "                 batch_size=batch_size,\n",
    "                 epochs=epochs,\n",
    "                 validation_data=(X_test_pad,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jk2LdiWW9Qex"
   },
   "outputs": [],
   "source": [
    "# Accuracy plot\n",
    "plt.plot(hist.history['accuracy'])\n",
    "plt.plot(hist.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Loss plot\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Vbn7Txp9Qez"
   },
   "source": [
    "## 4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8OPi0zqc9Qez"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test_pad)\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "predictions = [class_names[pred] for pred in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8dVDEKf09Qe2"
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy: {:.2f}%\".format(accuracy_score(data_test.Emotion, predictions) * 100))\n",
    "print(\"\\nF1 Score: {:.2f}\".format(f1_score(data_test.Emotion, predictions, average='micro') * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_qUbM9VI9Qe4"
   },
   "source": [
    "#### Plotting confusion Matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GCqt6fyh9Qe5"
   },
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 16})\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    '''\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    '''\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    # Set size\n",
    "    fig.set_size_inches(12.5, 7.5)\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.grid(False)\n",
    "    \n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qI8gnGcW9Qe7"
   },
   "outputs": [],
   "source": [
    "print(\"\\nF1 Score: {:.2f}\".format(f1_score(data_test.Emotion, predictions, average='micro') * 100))\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(data_test.Emotion, predictions, classes=class_names, normalize=True, title='Normalized confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AaPtAkF19Qe9"
   },
   "source": [
    "#### Let's try other inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U6vsehXd9Qe9"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/unified_dataset_cleaned.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Egdg2zyU9Qe_"
   },
   "outputs": [],
   "source": [
    "df = df.loc[df['Emotion'].isin(class_names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OON8S5A09QfB",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZD1JNaiq9QfD"
   },
   "outputs": [],
   "source": [
    "def deEmojify(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u_2lOwUo9QfG"
   },
   "outputs": [],
   "source": [
    "new_texts = []\n",
    "texts = df.Text.tolist()\n",
    "for i in range(len(texts)):\n",
    "    new_str = deEmojify(texts[i])\n",
    "    new_texts.append(new_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Miv_f4Fh9QfI"
   },
   "outputs": [],
   "source": [
    "new_texts = [t.lower() for t in new_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bmSB4zCa9QfK"
   },
   "outputs": [],
   "source": [
    "df.Text = new_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bs4VGC_t9QfM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SXRotd689QfO"
   },
   "outputs": [],
   "source": [
    "print('Message: {}\\nPredicted: {}'.format(X_test[4], predictions[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wB35DXBe9QfQ"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "message = ['delivery was hour late and my pizza was cold!']\n",
    "\n",
    "seq = tokenizer.texts_to_sequences(message)\n",
    "padded = pad_sequences(seq, maxlen=max_seq_len)\n",
    "\n",
    "start_time = time.time()\n",
    "pred = model.predict(padded)\n",
    "\n",
    "print('Message: ' + str(message))\n",
    "print('predicted: {} ({:.2f} seconds)'.format(class_names[np.argmax(pred)], (time.time() - start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-BJF9JDf9QfS"
   },
   "source": [
    "# Done\n",
    "Save the model for later use üôÉ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v6frpyzD9QfS"
   },
   "outputs": [],
   "source": [
    "# creates a HDF5 file 'my_model.h5'\n",
    "model.save('models/cnn_w2v.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MhgIGmXo9QfU"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "predictor = load_model('models/cnn_w2v.h5')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "cnn_w2v_wiki.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
